name: MLOps CI/CD Pipeline for Iris Dataset
description: This workflow automates the CI/CD pipeline for the Iris dataset project, including data loading, preprocessing, model training, MLflow tracking, and deployment.
versi: 2.0
author: Imran Khan
date: 2023-10-01

on:
  push:
    branches: [ main, imran ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      retrain_trigger:
        description: 'Triggered by new training data'
        required: false
        default: 'false'

env:
  DOCKER_IMAGE: ${{ github.repository }}
  DOCKER_TAG: ${{ github.sha }}
  DEPLOY_SERVER: ${{ secrets.DEPLOY_SERVER }}
  DEPLOY_USER: ${{ secrets.DEPLOY_USER }}

jobs:
  test:
    name: Code Quality & Testing
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install flake8 black isort pytest-cov
        
    - name: Code linting with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        
    - name: Code formatting check with black
      run: black --check --diff .
      
    - name: Import sorting check with isort
      run: isort --check-only --diff .
      
    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=src --cov=api --cov=db --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  data_loading:
    name: Data Loading & Validation
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Setup DVC
      uses: iterative/setup-dvc@v1
      
    - name: Configure DVC remote (using GitHub as remote)
      run: |
        dvc remote add -d origin https://github.com/${{ github.repository }}.git
        dvc remote modify origin auth github
        
    - name: Pull data with DVC
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        dvc pull || echo "No data to pull, will create initial dataset"
        
    - name: Run data loading
      run: |
        python -c "
        from src.data_pipeline import DataPipeline
        pipeline = DataPipeline()
        data = pipeline.load_and_preprocess()
        print('Data loading completed successfully')
        print(f'Train shape: {data[\"X_train\"].shape}')
        print(f'Test shape: {data[\"X_test\"].shape}')
        "
        
    - name: Validate data integrity
      run: |
        python -c "
        import pandas as pd
        from pathlib import Path
        
        # Check if data file exists and is valid
        data_file = Path('data/iris_full.csv')
        if data_file.exists():
            df = pd.read_csv(data_file)
            assert len(df) == 150, 'Iris dataset should have 150 samples'
            assert len(df.columns) == 6, 'Should have 4 features + target + target_name'
            print('Data validation passed')
        else:
            print('Data file not found, will be created in next step')
        "
    
    - name: Update DVC data version
      run: |
        if [ -f "data/iris_full.csv" ]; then
          dvc add data/iris_full.csv
          git add data/iris_full.csv.dvc data/.gitignore
        fi
        
    - name: Upload data artifacts
      uses: actions/upload-artifact@v3
      with:
        name: processed-data
        path: |
          data/
          models/scaler.pkl
        retention-days: 7

  data_preprocessing:
    name: Data Preprocessing & Feature Engineering
    runs-on: ubuntu-latest
    needs: data_loading
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download data artifacts
      uses: actions/download-artifact@v3
      with:
        name: processed-data
        
    - name: Run preprocessing pipeline
      run: |
        python -c "
        from src.data_pipeline import DataPipeline
        import numpy as np
        
        pipeline = DataPipeline()
        data = pipeline.load_and_preprocess()
        
        # Validate preprocessing
        assert data['X_train'].shape[1] == 4, 'Should have 4 features'
        assert np.abs(data['X_train'].mean()) < 1e-10, 'Data should be standardized'
        assert np.abs(data['X_train'].std() - 1.0) < 0.1, 'Data should have unit variance'
        
        print('Preprocessing validation passed')
        print(f'Train mean: {data[\"X_train\"].mean():.6f}')
        print(f'Train std: {data[\"X_train\"].std():.6f}')
        "
        
    - name: Upload preprocessing artifacts
      uses: actions/upload-artifact@v3
      with:
        name: preprocessed-data
        path: |
          data/
          models/
        retention-days: 7

  model_training:
    name: Model Training & Evaluation
    runs-on: ubuntu-latest
    needs: data_preprocessing
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download preprocessing artifacts
      uses: actions/download-artifact@v3
      with:
        name: preprocessed-data
        
    - name: Run model training
      run: |
        python -c "
        import os
        from pathlib import Path
        from src.data_pipeline import DataPipeline
        from src.model_pipeline import ModelPipeline
        
        # Set MLflow tracking URI
        mlruns_dir = Path('mlruns').resolve()
        mlruns_dir.mkdir(exist_ok=True)
        os.environ['MLFLOW_TRACKING_URI'] = f'file://{mlruns_dir}'
        
        # Run training pipeline
        data_pipeline = DataPipeline()
        model_pipeline = ModelPipeline()
        
        data = data_pipeline.load_and_preprocess()
        best_model = model_pipeline.train_and_evaluate(data)
        
        print(f'Training completed. Best model accuracy: {best_model[\"accuracy\"]:.4f}')
        "
        
    - name: Validate model artifacts
      run: |
        python -c "
        import json
        from pathlib import Path
        
        # Check metadata file
        metadata_file = Path('models/metadata.json')
        assert metadata_file.exists(), 'Metadata file should exist'
        
        with open(metadata_file) as f:
            metadata = json.load(f)
            
        assert 'best_model' in metadata, 'Metadata should contain best_model'
        assert 'best_accuracy' in metadata, 'Metadata should contain best_accuracy'
        assert metadata['best_accuracy'] > 0.8, 'Model accuracy should be > 80%'
        
        print(f'Model validation passed. Best model: {metadata[\"best_model\"]}')
        print(f'Best accuracy: {metadata[\"best_accuracy\"]:.4f}')
        "
        
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: trained-models
        path: |
          models/
          mlruns/
        retention-days: 30

  mlflow_tracking:
    name: MLflow Experiment Tracking
    runs-on: ubuntu-latest
    needs: model_training
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-models
        
    - name: Generate MLflow report
      run: |
        python -c "
        import os
        from pathlib import Path
        from src.mlflow_utils import MLflowManager
        
        # Set MLflow tracking URI
        mlruns_dir = Path('mlruns').resolve()
        os.environ['MLFLOW_TRACKING_URI'] = f'file://{mlruns_dir}'
        
        # Generate MLflow report
        mlflow_manager = MLflowManager()
        mlflow_manager.demonstrate_tracking()
        mlflow_manager.show_detailed_run_info()
        
        # Get experiment summary
        summary = mlflow_manager.get_experiment_summary()
        print('MLflow Experiment Summary:')
        print(f'Total runs: {summary.get(\"total_runs\", 0)}')
        print(f'Best accuracy: {summary.get(\"best_accuracy\", 0):.4f}')
        "
        
    - name: Upload MLflow artifacts
      uses: actions/upload-artifact@v3
      with:
        name: mlflow-tracking
        path: mlruns/
        retention-days: 90

  initialize_database:
    name: Initialize Database & Services
    runs-on: ubuntu-latest
    needs: mlflow_tracking
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-models
        
    - name: Initialize database and services
      run: |
        python -c "
        from db.database import DatabaseManager
        from src.inference_service import InferenceService
        
        # Initialize database
        db_manager = DatabaseManager()
        db_manager.init_db()
        print('Database initialized successfully')
        
        # Initialize inference service
        inference_service = InferenceService()
        success = inference_service.load_models()  # Load both models
        
        if success:
            print('Inference service initialized successfully')
            model_info = inference_service.get_models_info()
            for model_name, info in model_info.items():
                print(f'{model_name}: accuracy = {info[\"accuracy\"]:.4f}')
        else:
            raise Exception('Failed to initialize inference service')
        "
        
    - name: Test inference service
      run: |
        python -c "
        from src.inference_service import InferenceService
        
        inference_service = InferenceService()
        inference_service.load_models()
        
        # Test prediction
        test_features = [5.1, 3.5, 1.4, 0.2]
        
        # Test both models
        for model_name in ['logisticregression', 'randomforest']:
            result = inference_service.predict(test_features, model_name)
            print(f'{model_name.title()}: {result.prediction_name} (confidence: {result.confidence:.3f})')
        
        print('Inference service test passed')
        "

  build:
    name: Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: initialize_database
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-models
        
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Login to GitHub Container Registry
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Login to DockerHub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
      if: secrets.DOCKERHUB_USERNAME != ''
      
    - name: Build and push to GitHub Container Registry
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ghcr.io/${{ github.repository }}:latest
          ghcr.io/${{ github.repository }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Build and push to DockerHub
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKERHUB_USERNAME }}/mlops-iris:latest
          ${{ secrets.DOCKERHUB_USERNAME }}/mlops-iris:${{ github.sha }}
      if: secrets.DOCKERHUB_USERNAME != ''

  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup SSH
      uses: webfactory/ssh-agent@v0.8.0
      with:
        ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
        
    - name: Add server to known hosts
      run: |
        ssh-keyscan -H ${{ secrets.DEPLOY_SERVER }} >> ~/.ssh/known_hosts
        
    - name: Deploy to server
      run: |
        # Copy deployment script to server
        scp deploy.sh ${{ secrets.DEPLOY_USER }}@${{ secrets.DEPLOY_SERVER }}:/tmp/
        
        # Execute deployment script on server
        ssh ${{ secrets.DEPLOY_USER }}@${{ secrets.DEPLOY_SERVER }} "
          chmod +x /tmp/deploy.sh
          DOCKER_IMAGE=ghcr.io/${{ github.repository }}:${{ github.sha }} /tmp/deploy.sh
        "
        
    - name: Verify deployment
      run: |
        # Wait for service to start
        sleep 30
        
        # Health check
        ssh ${{ secrets.DEPLOY_USER }}@${{ secrets.DEPLOY_SERVER }} "
          curl -f http://localhost:8000/health || exit 1
          echo 'Deployment verification passed'
        "
        
    - name: Cleanup
      run: |
        ssh ${{ secrets.DEPLOY_USER }}@${{ secrets.DEPLOY_SERVER }} "
          rm -f /tmp/deploy.sh
          docker system prune -f
        "